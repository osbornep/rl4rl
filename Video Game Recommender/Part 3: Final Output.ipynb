{"cells":[{"metadata":{"_uuid":"649383e1e0ae79464e4984e8a3934d3e180a1baa"},"cell_type":"markdown","source":["# AI in Video Games: Improving Decision Making in League of Legends using Real Match Statistics and Personal Preferences\n","\n","## Part 3: Introducing Personal Preferences and Concept of Final Output\n","\n","This is the final part of a short series where I transformed the competitive match statistics into an MDP and applied reinforcement learning to find the optimal play given a current state.\n","\n","Write up can be found on Medium page or my website by following the links:\n","\n","https://medium.com/@philiposbornedata\n","\n","or \n","\n","https://www.philiposbornedata.com/\n","\n","\n","Please let me know if you have any questions.\n","\n","Thanks\n","Phil\n"]},{"metadata":{"_uuid":"e658de9fdee3386430d3bb770e8711fd0b01fc43"},"cell_type":"markdown","source":["### Motivations and Objectives\n","League of Legends is a team oriented video game where on two team teams (with 5 players in each) compete for objectives and kills. Gaining an advantage enables the players to become stronger (obtain better items and level up faster) than their opponents and, as their advantage increases, the likelihood of winning the game also increases. We therefore have a sequence of events dependent on previous events that lead to one team destroying the other’s base and winning the game. \n","\n","Sequences like this being modelled statistically is nothing new; for years now researchers have considered how this is applied in sports, such as basketball (https://arxiv.org/pdf/1507.01816.pdf), where a sequence of passing, dribbling and foul plays lead to a team obtaining or losing points. The aim of research such as this one mentioned is to provide more detailed insight beyond a simple box score (number of points or kill gained by player in basketball or video games respectively) and consider how teams perform when modelled as a sequence of events connected in time. \n","\n","Modelling the events in this way is even more important in games such as League of Legends as taking objectives and kills lead towards both an item and level advantage. For example, a player obtaining the first kill of the game nets them gold that can be used to purchase more powerful items. With this item they are then strong enough to obtain more kills and so on until they can lead their team to a win. Facilitating a lead like this is often referred to as ‘snowballing’ as the players cumulatively gain advantages but often games are not this one sided and objects and team plays are more important. \n","\n","#### The aim of this is project is simple; can we calculate the next best event given what has occurred previously in the game so that the likelihood of eventually leading to a win increases based on real match statistics?\n","\n","However, there are many factors that lead to a player’s decision making in a game that cannot be easily measured. No how matter how much data collected, the amount of information a player can capture is beyond any that a computer can detect (at least for now!). For example, players may be over or underperforming in this game or may simply have a preference for the way they play (often defined by the types of characters they play). Some players will naturally be more aggressive and look for kills while others will play passively and push for objectives instead.\n","Therefore, we further develop our model to allow the player to adjust the recommended play on their preferences.\n","\n"]},{"metadata":{"_uuid":"9103566038ee06e0129ebb5667eacc352a4dec92"},"cell_type":"markdown","source":["### What makes our model 'Artifical Intellegence'?\n","\n","In out first part, we performed some introductory statistical analysis. For example, we were able to calculate the probability of winning given the team obtains the first and second objective in a match as shown in the image below.\n","\n","There are two components that make takes our project beyond simple statistics into AI:\n","\n","- First, the model learns which actions are best with no pre-conceived notion of the game and \n","- Secondly, it attempts to learn a player's preference for decisions that will influence the model's output.\n","\n","How we define our Markov Decision Process and collect a player's preference will define what our model learns and therefore outputs. \n"]},{"metadata":{"_uuid":"a7a62945759d0c9755cac4541070f5d953f46e64"},"cell_type":"markdown","source":["![Probability of Winning Given Outcome of First Two Events](https://i.imgur.com/p3gb0GC.png)"]},{"metadata":{"_uuid":"166d34e705cd454bddf2f9cd5e21c6b87f6a1a7a"},"cell_type":"markdown","source":["## Pre-Processing and Creating Markov Decision Process from Match Statistics"]},{"metadata":{"_uuid":"82884c8746974c79bcfda9fb1843417033fe588c"},"cell_type":"markdown","source":["### AI Model II: Introducing Gold Difference\n","\n","I then realised from the results of our first model attempts that we have nothing to take into account the cumulative impact negative and positive events have on the likelihood in later states. In other words, the current MDP probabilities are just as likely to happen whether you are ahead or behind at that point in time. In the game this simply isn’t true; if you are behind then kills, structures and other objectives are much harder to obtain and we need to account for this. \n","Therefore, we introduce gold difference between the teams as a way to redefine our states. We now aim to have a MDP defining the states to be both the order events occurred but also whether the team is behind, even or ahead in gold. We have categorised the gold difference to the following:\n","-\tEven: 0-999 gold difference (0-200 per player avg.)\n","-\tSlightly Behind/Ahead: 1,000-2,499 gold difference (200-500 per player avg.)\n","-\tBehind/Ahead: 2,500-4,999 gold difference (500-1,000 per player avg.)\n","-\tVery Behind/Ahead: 5,000 gold difference (1,000+ per player avg.) \n","We also now consider no events to be of interest and include this as ‘NONE’ event so that each minute has at least one event. This ‘NONE’ event represents if a team decided to try stalling game and helps differentiate teams that are better at obtaining a gold lead in the early game without kills or objectives (through minion kills). However, doing this also massively stretches our data thin as we have now added 7 categories to fit the available matches into but if we had access to more normal matches the amount of data would be sufficient. \n","As before, we can outline each step by the following:\n","\n"]},{"metadata":{"_uuid":"fa65cc1025911f555941f33f49d70a15ba090b80"},"cell_type":"markdown","source":["![MDP Example](https://i.imgur.com/JRfOiyf.png)"]},{"metadata":{"_uuid":"cf6e31d57fa99fc3c93bdfc74df8485cd9fe063f"},"cell_type":"markdown","source":["#### Pre-processing\n","\n","1.\tImport data for kills, structures, monsters and gold difference.\n","2.\tConvert ‘Address’ into an id feature.\n","3.\tRemove all games with old dragon.\n","4.\tStart with gold difference data and aggregate this by minute of event, match id and team that made event as before\n","5.\tAppend (stack) the kills, monsters and structures data onto the end of this creating a row for each event and sort by time event occurred (avg. for kills).\n","6.\tAdd and ‘Event Number’ feature that shows the order of events in each of the matches.\n","7.\tCreate a consolidated ‘Event’ feature with either kills, structures, monsters or ‘NONE’ for each event on the row.\n","8.\tTransform this into one row per match with columns now denoting each event.\n","9.\tOnly consider red team’s perspective so merge columns and where blue gains become negative red gains. Also add on game length and outcome for red team.\n","10.\tReplace all blank values (i.e. game ended in earlier step) with the game outcome for the match so that the last event in all rows is the match outcome.\n","11.\tTransform into MDP where we have P( X_t | X_t-1 ) for all event types in between each event number and state defined by gold difference.\n"]},{"metadata":{"_uuid":"064eff0d2a00cd41556ca6ed4336a697ccb86fd0"},"cell_type":"markdown","source":["### Import Packages and Data"]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"99079d6647a88d85ba4334c8b25f397005e3dff0"},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import Image\n","import math\n","from scipy.stats import kendalltau\n","\n","from IPython.display import clear_output\n","import timeit\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"830c80409bb8af5219e1f0d1f378052c9596a28d"},"cell_type":"code","source":["kills = pd.read_csv('./data/kills.csv')\n","matchinfo = pd.read_csv('./data/matchinfo.csv')\n","monsters = pd.read_csv('./data/monsters.csv')\n","structures = pd.read_csv('./data/structures.csv')\n","\n","#kills = pd.read_csv('../input/kills.csv')\n","#matchinfo = pd.read_csv('../input/matchinfo.csv')\n","#monsters = pd.read_csv('../input/monsters.csv')\n","#structures = pd.read_csv('../input/structures.csv')\n","\n","\n","gold = pd.read_csv('./data/gold.csv')\n","#gold = pd.read_csv('../input/gold.csv')"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cd83a6754a5be9b3327d08a192cdb9e30ed1c04"},"cell_type":"markdown","source":["### Create MDP from Match Statistics (see part 1 or 2 for more details)"]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d9f13b22b0302a3a0a1221dc23f83ef6c854e99f"},"cell_type":"code","source":["# Add ID column based on last 16 digits in match address for simpler matching\n","gold = gold[gold['Type']==\"golddiff\"]\n","\n","matchinfo['id'] = matchinfo['Address'].astype(str).str[-16:]\n","kills['id'] = kills['Address'].astype(str).str[-16:]\n","monsters['id'] = monsters['Address'].astype(str).str[-16:]\n","structures['id'] = structures['Address'].astype(str).str[-16:]\n","gold['id'] = gold['Address'].astype(str).str[-16:]\n","\n","\n","\n","# Dragon became multiple types in patch v6.9 (http://leagueoflegends.wikia.com/wiki/V6.9) \n","# so we remove and games before this change occured and only use games with the new dragon system\n","\n","old_dragon_id = monsters[ monsters['Type']==\"DRAGON\"]['id'].unique()\n","old_dragon_id\n","\n","monsters = monsters[ ~monsters['id'].isin(old_dragon_id)]\n","monsters = monsters.reset_index()\n","\n","matchinfo = matchinfo[ ~matchinfo['id'].isin(old_dragon_id)]\n","matchinfo = matchinfo.reset_index()\n","\n","kills = kills[ ~kills['id'].isin(old_dragon_id)]\n","kills = kills.reset_index()\n","\n","structures = structures[ ~structures['id'].isin(old_dragon_id)]\n","structures = structures.reset_index()\n","\n","gold = gold[ ~gold['id'].isin(old_dragon_id)]\n","gold = gold.reset_index()\n","\n","#Transpose Gold table, columns become matches and rows become minutes\n","\n","gold_T = gold.iloc[:,3:-1].transpose()\n","\n","\n","gold2 = pd.DataFrame()\n","\n","start = timeit.default_timer()\n","for r in range(0,len(gold)):\n","    clear_output(wait=True)\n","    \n","    # Select each match column, drop any na rows and find the match id from original gold table\n","    gold_row = gold_T.iloc[:,r]\n","    gold_row = gold_row.dropna()\n","    gold_row_id = gold['id'][r]\n","    \n","    # Append into table so that each match and event is stacked on top of one another    \n","    gold2 = gold2.append(pd.DataFrame({'id':gold_row_id,'GoldDiff':gold_row}))\n","    \n","    \n","    stop = timeit.default_timer()\n","   \n","                 \n","    print(\"Current progress:\",np.round(r/len(gold) *100, 2),\"%\")        \n","    print(\"Current run time:\",np.round((stop - start)/60,2),\"minutes\")\n","        \n","\n","gold3 = gold2[['id','GoldDiff']]\n","\n","### Create minute column with index, convert from 'min_1' to just the number\n","gold3['Minute'] = gold3.index.to_series()\n","gold3['Minute'] = np.where(gold3['Minute'].str[-2]==\"_\", gold3['Minute'].str[-1],gold3['Minute'].str[-2:])\n","gold3[gold3['Minute']==\"pe\"]\n","\n","gold3 = gold3.iloc[1:,]\n","gold3['Minute'] = gold3['Minute'].astype(int)\n","gold3 = gold3.reset_index()\n","gold3 = gold3.sort_values(by=['id','Minute'])\n","\n","\n","\n","# Gold difference from data is relative to blue team's perspective,\n","# therefore we reverse this by simply multiplying amount by -1\n","gold3['GoldDiff'] = gold3['GoldDiff']*-1\n","\n","\n","gold4 = gold3\n","\n","matchinfo2 = matchinfo[['id','rResult','gamelength']]\n","matchinfo2['gamlength'] = matchinfo2['gamelength'] + 1\n","matchinfo2['index'] = 'min_'+matchinfo2['gamelength'].astype(str)\n","matchinfo2['rResult2'] =  np.where(matchinfo2['rResult']==1,999999,-999999)\n","matchinfo2 = matchinfo2[['index','id','rResult2','gamelength']]\n","matchinfo2.columns = ['index','id','GoldDiff','Minute']\n","\n","\n","gold4 = gold4.append(matchinfo2)\n","\n","\n","kills = kills[ kills['Time']>0]\n","\n","kills['Minute'] = kills['Time'].astype(int)\n","\n","kills['Team'] = np.where( kills['Team']==\"rKills\",\"Red\",\"Blue\")\n","\n","# For the Kills table, we need decided to group by the minute in which the kills took place and averaged \n","# the time of the kills which we use later for the order of events\n","\n","f = {'Time':['mean','count']}\n","\n","killsGrouped = kills.groupby( ['id','Team','Minute'] ).agg(f).reset_index()\n","killsGrouped.columns = ['id','Team','Minute','Time Avg','Count']\n","killsGrouped = killsGrouped.sort_values(by=['id','Minute'])\n","\n","\n","structures = structures[ structures['Time']>0]\n","\n","structures['Minute'] = structures['Time'].astype(int)\n","structures['Team'] = np.where(structures['Team']==\"bTowers\",\"Blue\",\n","                        np.where(structures['Team']==\"binhibs\",\"Blue\",\"Red\"))\n","structures2 = structures.sort_values(by=['id','Minute'])\n","\n","structures2 = structures2[['id','Team','Time','Minute','Type']]\n","\n","\n","monsters['Type2'] = np.where( monsters['Type']==\"FIRE_DRAGON\", \"DRAGON\",\n","                    np.where( monsters['Type']==\"EARTH_DRAGON\",\"DRAGON\",\n","                    np.where( monsters['Type']==\"WATER_DRAGON\",\"DRAGON\",       \n","                    np.where( monsters['Type']==\"AIR_DRAGON\",\"DRAGON\",   \n","                             monsters['Type']))))\n","\n","monsters = monsters[ monsters['Time']>0]\n","\n","monsters['Minute'] = monsters['Time'].astype(int)\n","\n","monsters['Team'] = np.where( monsters['Team']==\"bDragons\",\"Blue\",\n","                   np.where( monsters['Team']==\"bHeralds\",\"Blue\",\n","                   np.where( monsters['Team']==\"bBarons\", \"Blue\", \n","                           \"Red\")))\n","\n","monsters = monsters[['id','Team','Time','Minute','Type2']]\n","monsters.columns = ['id','Team','Time','Minute','Type']\n","\n","\n","GoldstackedData = gold4.merge(killsGrouped, how='left',on=['id','Minute'])\n"," \n","monsters_structures_stacked = structures2.append(monsters[['id','Team','Minute','Time','Type']])\n","\n","GoldstackedData2 = GoldstackedData.merge(monsters_structures_stacked, how='left',on=['id','Minute'])\n","\n","GoldstackedData2 = GoldstackedData2.sort_values(by=['id','Minute'])\n","\n","GoldstackedData3 = GoldstackedData2\n","GoldstackedData3['Time2'] = GoldstackedData3['Time'].fillna(GoldstackedData3['Time Avg']).fillna(GoldstackedData3['Minute'])\n","GoldstackedData3['Team'] = GoldstackedData3['Team_x'].fillna(GoldstackedData3['Team_y'])\n","GoldstackedData3 = GoldstackedData3.sort_values(by=['id','Time2'])\n","\n","GoldstackedData3['EventNum'] = GoldstackedData3.groupby('id').cumcount()+1\n","\n","GoldstackedData3 = GoldstackedData3[['id','EventNum','Team','Minute','Time2','GoldDiff','Count','Type']]\n","\n","GoldstackedData3.columns = ['id','EventNum','Team','Minute','Time','GoldDiff','KillCount','Struct/Monster']\n","\n","\n","# We then add an 'Event' column to merge the columns into one, where kills are now\n","# simple labelled as 'KILLS'\n","\n","GoldstackedData3['Event'] = np.where(GoldstackedData3['KillCount']>0,\"KILLS\",None)\n","GoldstackedData3['Event'] = GoldstackedData3['Event'].fillna(GoldstackedData3['Struct/Monster'])\n","\n","GoldstackedData3['Event'] = GoldstackedData3['Event'].fillna(\"NONE\")\n","\n","GoldstackedData3['GoldDiff2'] = np.where( GoldstackedData3['GoldDiff']== 999999,\"WIN\",\n","                                np.where( GoldstackedData3['GoldDiff']==-999999, 'LOSS',\n","                                         \n","    \n","                                np.where((GoldstackedData3['GoldDiff']<1000) & (GoldstackedData3['GoldDiff']>-1000),\n","                                        \"EVEN\",\n","                                np.where( (GoldstackedData3['GoldDiff']>=1000) & (GoldstackedData3['GoldDiff']<2500),\n","                                         \"SLIGHTLY_AHEAD\",\n","                                np.where( (GoldstackedData3['GoldDiff']>=2500) & (GoldstackedData3['GoldDiff']<5000),\n","                                         \"AHEAD\",\n","                                np.where( (GoldstackedData3['GoldDiff']>=5000),\n","                                         \"VERY_AHEAD\",\n","                                         \n","                                np.where( (GoldstackedData3['GoldDiff']<=-1000) & (GoldstackedData3['GoldDiff']>-2500),\n","                                         \"SLIGHTLY_BEHIND\",\n","                                np.where( (GoldstackedData3['GoldDiff']<=-2500) & (GoldstackedData3['GoldDiff']>-5000),\n","                                         \"BEHIND\",\n","                                np.where( (GoldstackedData3['GoldDiff']<=-5000),\n","                                         \"VERY_BEHIND\",\"ERROR\"\n","                                        \n","                                        )))))))))\n","\n","GoldstackedData3['Next_Min'] = GoldstackedData3['Minute']+1\n","\n","\n","GoldstackedData4 = GoldstackedData3.merge(gold4[['id','Minute','GoldDiff']],how='left',left_on=['id','Next_Min'],\n","                                         right_on=['id','Minute'])\n","\n","GoldstackedData4['GoldDiff2_Next'] =  np.where( GoldstackedData4['GoldDiff_y']== 999999,\"WIN\",\n","                                np.where( GoldstackedData4['GoldDiff_y']==-999999, 'LOSS',\n","                                         \n","    \n","                                np.where((GoldstackedData4['GoldDiff_y']<1000) & (GoldstackedData4['GoldDiff_y']>-1000),\n","                                        \"EVEN\",\n","                                np.where( (GoldstackedData4['GoldDiff_y']>=1000) & (GoldstackedData4['GoldDiff_y']<2500),\n","                                         \"SLIGHTLY_AHEAD\",\n","                                np.where( (GoldstackedData4['GoldDiff_y']>=2500) & (GoldstackedData4['GoldDiff_y']<5000),\n","                                         \"AHEAD\",\n","                                np.where( (GoldstackedData4['GoldDiff_y']>=5000),\n","                                         \"VERY_AHEAD\",\n","                                         \n","                                np.where( (GoldstackedData4['GoldDiff_y']<=-1000) & (GoldstackedData4['GoldDiff_y']>-2500),\n","                                         \"SLIGHTLY_BEHIND\",\n","                                np.where( (GoldstackedData4['GoldDiff_y']<=-2500) & (GoldstackedData4['GoldDiff_y']>-5000),\n","                                         \"BEHIND\",\n","                                np.where( (GoldstackedData4['GoldDiff_y']<=-5000),\n","                                         \"VERY_BEHIND\",\"ERROR\"\n","                                        \n","                                        )))))))))\n","GoldstackedData4 = GoldstackedData4[['id','EventNum','Team','Minute_x','Time','Event','GoldDiff2','GoldDiff2_Next']]\n","GoldstackedData4.columns = ['id','EventNum','Team','Minute','Time','Event','GoldDiff2','GoldDiff2_Next']\n","\n","GoldstackedData4['Event'] = np.where( GoldstackedData4['Team']==\"Red\", \"+\"+GoldstackedData4['Event'],\n","                                np.where(GoldstackedData4['Team']==\"Blue\", \"-\"+GoldstackedData4['Event'], \n","                                         GoldstackedData4['Event']))\n","\n","\n","\n","\n","\n","# Errors are caused due to game ending in minute and then there is no 'next_min' info for this game but our method expects there to be\n","GoldstackedData4 = GoldstackedData4[GoldstackedData4['GoldDiff2_Next']!=\"ERROR\"]\n","GoldstackedData4[GoldstackedData4['GoldDiff2_Next']==\"ERROR\"]\n","\n","\n","GoldstackedDataFINAL = GoldstackedData4\n","GoldstackedDataFINAL['Min_State_Action_End'] = ((GoldstackedDataFINAL['Minute'].astype(str)) + \"_\"\n","                                       + (GoldstackedDataFINAL['GoldDiff2'].astype(str)) + \"_\"\n","                                       + (GoldstackedDataFINAL['Event'].astype(str)) + \"_\"  \n","                                       + (GoldstackedDataFINAL['GoldDiff2_Next'].astype(str))\n","                                      )\n","\n","GoldstackedDataFINAL['MSAE'] = ((GoldstackedDataFINAL['Minute'].astype(str)) + \"_\"\n","                                       + (GoldstackedDataFINAL['GoldDiff2'].astype(str)) + \"_\"\n","                                       + (GoldstackedDataFINAL['Event'].astype(str)) + \"_\"  \n","                                       + (GoldstackedDataFINAL['GoldDiff2_Next'].astype(str))\n","                                      )\n","\n","\n","goldMDP = GoldstackedDataFINAL[['Minute','GoldDiff2','Event','GoldDiff2_Next']]\n","goldMDP.columns = ['Minute','State','Action','End']\n","goldMDP['Counter'] = 1\n","\n","goldMDP2 = goldMDP.groupby(['Minute','State','Action','End']).count().reset_index()\n","goldMDP2['Prob'] = goldMDP2['Counter']/(goldMDP2['Counter'].sum())\n","\n","goldMDP3 = goldMDP.groupby(['Minute','State','Action']).count().reset_index()\n","goldMDP3['Prob'] = goldMDP3['Counter']/(goldMDP3['Counter'].sum())\n","\n","\n","\n","goldMDP4 = goldMDP2.merge(goldMDP3[['Minute','State','Action','Prob']], how='left',on=['Minute','State','Action'] )\n","\n","goldMDP4['GivenProb'] = goldMDP4['Prob_x']/goldMDP4['Prob_y']\n","goldMDP4 = goldMDP4.sort_values('GivenProb',ascending=False)\n","goldMDP4['Next_Minute'] = goldMDP4['Minute']+1\n","    \n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0b18f49a605ddd27d5b4b3f6b7983fc96383a2be"},"cell_type":"code","source":["goldMDP4.sort_values(['Minute','State']).head(20)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbc1083a6f951a5e2ac84986f8d10edcc911256b"},"cell_type":"markdown","source":["### Introducing Preferences with Rewards\n","\n","First, we adjust our model code to include the reward in our Return calculation. Then, when we run the model, instead of simply having a reward equal to zero, we now introduce a bias towards some actions.\n","\n","In our first example, we show what happens if we heavily weight an action positively and then, in our second, if we weight an action negatively. \n","\n","#### Model v6 Pseudocode in Plain English\n","\n","Our very final version of the model can be simply summarised by the following:\n","\n","\n","1.\tIntroduce parameters\n","2.\tInitialise start state, start event and start action\n","3.\tSelect actions based on either first provided or randomly over their likelihood of occurring as defined in MDP\n","4.\tWhen action reaches win/loss, end episode\n","5.\tTrack the actions taken in the episode and final outcome (win/loss)\n","6.\tRepeat for x number of episodes\n"]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4c5511cefb45abdf8871035fd556c2b4ee326324"},"cell_type":"code","source":["def MCModelv6(data, alpha, gamma, epsilon, reward, StartState, StartMin, StartAction, num_episodes, Max_Mins):\n","    \n","    # Initiatise variables appropiately\n","    \n","    data['V'] = 0\n","    data_output = data\n","    \n","    outcomes = pd.DataFrame()\n","    episode_return = pd.DataFrame()\n","    actions_output = pd.DataFrame()\n","    V_output = pd.DataFrame()\n","    \n","    \n","    Actionist = [\n","       'NONE',\n","       'KILLS', 'OUTER_TURRET', 'DRAGON', 'RIFT_HERALD', 'BARON_NASHOR',\n","       'INNER_TURRET', 'BASE_TURRET', 'INHIBITOR', 'NEXUS_TURRET',\n","       'ELDER_DRAGON'] \n","        \n","    for e in range(0,num_episodes):\n","        clear_output(wait=True)\n","        \n","        action = []\n","        \n","        current_min = StartMin\n","        current_state = StartState\n","        \n","        \n","        \n","        data_e1 = data\n","    \n","    \n","        actions = pd.DataFrame()\n","\n","        for a in range(0,100):\n","            \n","            action_table = pd.DataFrame()\n","       \n","            # Break condition if game ends or gets to a large number of mins \n","            if (current_state==\"WIN\") | (current_state==\"LOSS\") | (current_min==Max_Mins):\n","                continue\n","            else:\n","                if a==0:\n","                    data_e1=data_e1\n","                   \n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"+RIFT_HERALD\"])==1):\n","                    data_e1_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n","                    \n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"-RIFT_HERALD\"])==1):\n","                    data_e1 = data_e1[(data_e1['Action']!='+RIFT_HERALD')|(data_e1['Action']!='-RIFT_HERALD')]\n","                \n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"+OUTER_TURRET\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='+OUTER_TURRET']\n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"-OUTER_TURRET\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='-OUTER_TURRET']\n","                    \n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INNER_TURRET\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='+INNER_TURRET']\n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INNER_TURRET\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='-INNER_TURRET']\n","                    \n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"+BASE_TURRET\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='+BASE_TURRET']\n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"-BASE_TURRET\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='-BASE_TURRET']\n","                    \n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"+INHIBITOR\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='+INHIBITOR']\n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"-INHIBITOR\"])==3):\n","                    data_e1 = data_e1[data_e1['Action']!='-INHIBITOR']\n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"+NEXUS_TURRET\"])==2):\n","                    data_e1 = data_e1[data_e1['Action']!='+NEXUS_TURRET']\n","                elif (len(individual_actions_count[individual_actions_count['Action']==\"-NEXUS_TURRET\"])==2):\n","                    data_e1 = data_e1[data_e1['Action']!='-NEXUS_TURRET']\n","                \n","                       \n","                else:\n","                    data_e1 = data_e1\n","                    \n","                # Break condition if we do not have enough data    \n","                if len(data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)])==0:\n","                    continue\n","                else:             \n","\n","                    \n","                    # Greedy Selection:\n","                    # If this is our first action and start action is non, select greedily. \n","                    # Else, if first actions is given in our input then we use this as our start action. \n","                    # Else for other actions, if it is the first episode then we have no knowledge so randomly select actions\n","                    # Else for other actions, we randomly select actions a percentage of the time based on our epsilon and greedily (max V) for the rest \n","                    \n","                    \n","                    if   (a==0) & (StartAction is None):\n","                        random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n","                        random_action = random_action.reset_index()\n","                        current_action = random_action['Action'][0]\n","                    elif (a==0):\n","                        current_action =  StartAction\n","                    \n","                    elif (e==0) & (a>0):\n","                        random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n","                        random_action = random_action.reset_index()\n","                        current_action = random_action['Action'][0]\n","                    \n","                    elif (e>0) & (a>0):\n","                        epsilon = epsilon\n","                        greedy_rng = np.round(np.random.random(),2)\n","                        if (greedy_rng<=epsilon):\n","                            random_action = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)].sample()\n","                            random_action = random_action.reset_index()\n","                            current_action = random_action['Action'][0]\n","                        else:\n","                            greedy_action = (\n","                            \n","                                data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)][\n","                                    \n","                                    data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)]['V']==data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)]['V'].max()\n","                                \n","                                ])\n","                                \n","                            greedy_action = greedy_action.reset_index()\n","                            current_action = greedy_action['Action'][0]\n","                            \n","                  \n","                    \n","                        \n","\n","                    data_e = data_e1[(data_e1['Minute']==current_min)&(data_e1['State']==current_state)&(data_e1['Action']==current_action)]\n","\n","                    data_e = data_e[data_e['GivenProb']>0]\n","\n","\n","\n","\n","\n","                    data_e = data_e.sort_values('GivenProb')\n","                    data_e['CumProb'] = data_e['GivenProb'].cumsum()\n","                    data_e['CumProb'] = np.round(data_e['CumProb'],4)\n","\n","\n","                    rng = np.round(np.random.random()*data_e['CumProb'].max(),4)\n","                    action_table = data_e[ data_e['CumProb'] >= rng]\n","                    action_table = action_table[ action_table['CumProb'] == action_table['CumProb'].min()]\n","                    action_table = action_table.reset_index()\n","\n","\n","                    action = current_action\n","                    next_state = action_table['End'][0]\n","                    next_min = current_min+1\n","\n","\n","                    if next_state == \"WIN\":\n","                        step_reward = 10*(gamma**a)\n","                    elif next_state == \"LOSS\":\n","                        step_reward = -10*(gamma**a)\n","                    else:\n","                        step_reward = action_table['Reward']*(gamma**a)\n","\n","                    action_table['StepReward'] = step_reward\n","\n","\n","                    action_table['Episode'] = e\n","                    action_table['Action_Num'] = a\n","\n","                    current_action = action\n","                    current_min = next_min\n","                    current_state = next_state\n","\n","\n","                    actions = actions.append(action_table)\n","\n","                    individual_actions_count = actions\n","                    \n","        print(\"Current progress:\", np.round((e/num_episodes)*100,2),\"%\")\n","\n","        actions_output = actions_output.append(actions)\n","                \n","        episode_return = actions['StepReward'].sum()\n","\n","                \n","        actions['Return']= episode_return\n","                \n","        data_output = data_output.merge(actions[['Minute','State','Action','End','Return']], how='left',on =['Minute','State','Action','End'])\n","        data_output['Return'] = data_output['Return'].fillna(0)    \n","             \n","            \n","        data_output['V'] = np.where(data_output['Return']==0,data_output['V'],data_output['V'] + alpha*(data_output['Return']-data_output['V']))\n","        \n","        data_output = data_output.drop('Return', 1)\n","\n","        \n","        for actions in data_output[(data_output['Minute']==StartMin)&(data_output['State']==StartState)]['Action'].unique():\n","            V_outputs = pd.DataFrame({'Index':[str(e)+'_'+str(actions)],'Episode':e,'StartMin':StartMin,'StartState':StartState,'Action':actions,\n","                                      'V':data_output[(data_output['Minute']==StartMin)&(data_output['State']==StartState)&(data_output['Action']==actions)]['V'].sum()\n","                                     })\n","            V_output = V_output.append(V_outputs)\n","        \n","        if current_state==\"WIN\":\n","            outcome = \"WIN\"\n","        elif current_state==\"LOSS\":\n","            outcome = \"LOSS\"\n","        else:\n","            outcome = \"INCOMPLETE\"\n","        outcome = pd.DataFrame({'Epsiode':[e],'Outcome':[outcome]})\n","        outcomes = outcomes.append(outcome)\n","\n","        \n","   \n","\n","\n","    return(outcomes,actions_output,data_output,V_output)\n","    "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"af2bf5d12c2a5ec78116794dc2283bbef0172804"},"cell_type":"code","source":["alpha = 0.3\n","gamma = 0.9\n","num_episodes = 1000\n","epsilon = 0.2\n","\n","\n","goldMDP4['Reward'] = np.where(goldMDP4['Action']==\"+KILLS\",5,-0.005)\n","reward = goldMDP4['Reward']\n","\n","StartMin = 15\n","StartState = 'EVEN'\n","StartAction = None\n","data = goldMDP4\n","\n","Max_Mins = 50\n","start_time = timeit.default_timer()\n","\n","\n","Mdl6 = MCModelv6(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n","                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n","                num_episodes = num_episodes, Max_Mins = Max_Mins)\n","\n","elapsed = timeit.default_timer() - start_time\n","\n","print(\"Time taken to run model:\",np.round(elapsed/60,2),\"mins\")\n","print(\"Avg Time taken per episode:\", np.round(elapsed/num_episodes,2),\"secs\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"eaf93d4f7e3ce61c4640ace6ce912671928b2268"},"cell_type":"code","source":["final_output = Mdl6[2]\n","V_episodes = Mdl6[3]\n","\n","final_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)]\n","final_output3 = final_output2.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\n","final_output3[['Minute','State','Action','V']]\n","\n","single_action1 = final_output3['Action'][0]\n","single_action2 = final_output3['Action'][len(final_output3)-1]\n","\n","plot_data1 = V_episodes[(V_episodes['Action']==single_action1)]\n","plot_data2 = V_episodes[(V_episodes['Action']==single_action2)]\n","\n","plt.plot(plot_data1['Episode'],plot_data1['V'], label = single_action1, color = 'C2')\n","plt.plot(plot_data2['Episode'],plot_data2['V'], label = single_action2, color = 'C1')\n","plt.xlabel(\"Epsiode\")\n","plt.ylabel(\"V\")\n","plt.legend()\n","plt.title(\"V by Episode for the Best/Worst Actions given the Current State\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"95650043886f4ab78bc6bc56ab4c5c05d8c66b02"},"cell_type":"code","source":["alpha = 0.3\n","gamma = 0.9\n","num_episodes = 1000\n","epsilon = 0.2\n","\n","\n","goldMDP4['Reward'] = np.where(goldMDP4['Action']==\"+KILLS\",-5,-0.005)\n","reward = goldMDP4['Reward']\n","\n","StartMin = 15\n","StartState = 'EVEN'\n","StartAction = None\n","data = goldMDP4\n","\n","Max_Mins = 50\n","start_time = timeit.default_timer()\n","\n","\n","Mdl6_2 = MCModelv6(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n","                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n","                num_episodes = num_episodes, Max_Mins = Max_Mins)\n","\n","elapsed = timeit.default_timer() - start_time\n","\n","print(\"Time taken to run model:\",np.round(elapsed/60,2),\"mins\")\n","print(\"Avg Time taken per episode:\", np.round(elapsed/num_episodes,2),\"secs\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"878de71253f006f1e11b2185f16e228582967139"},"cell_type":"code","source":["final_output_2 = Mdl6_2[2]\n","V_episodes_2 = Mdl6_2[3]\n","\n","\n","final_output_22 = final_output_2[(final_output_2['Minute']==StartMin)&(final_output_2['State']==StartState)]\n","final_output_23 = final_output_22.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\n","final_output_23[['Minute','State','Action','V']]\n","\n","single_action1_2 = final_output_23['Action'][0]\n","single_action2_2 = final_output_23['Action'][len(final_output_23)-1]\n","\n","plot_data1_2 = V_episodes_2[(V_episodes_2['Action']==single_action1_2)]\n","plot_data2_2 = V_episodes_2[(V_episodes_2['Action']==single_action2_2)]\n","\n","plt.plot(plot_data1_2['Episode'],plot_data1_2['V'], label = single_action1_2, color = 'C1')\n","plt.plot(plot_data2_2['Episode'],plot_data2_2['V'], label = single_action2_2, color = 'C2')\n","plt.xlabel(\"Epsiode\")\n","plt.ylabel(\"V\")\n","plt.legend()\n","plt.title(\"V by Episode for the Best/Worst Actions given the Current State\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1bcc12544bad68bc7ef7d1b0e254de9ee27cb91"},"cell_type":"markdown","source":["### More realistic player preferences\n","\n","So let us attempt to approximately simulate a player's actual preferences. In this case, I have randomised some of the rewards to follow the two rules:\n","\n","1. The player doesn't want to give up any objectives\n","2. The player prioritises gaining objectives over kills\n","\n","\n","Therefore, our rewards for kills and losing objects are all the minimum of -0.05 whereas the other actions are randomised between -0.05 and 0.05."]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b282c41af7b083307aac17a941d62f1e84b8ff75"},"cell_type":"code","source":["goldMDP4['Reward'] = np.where(goldMDP4['Action']=='NONE',-0.05,        \n","                     np.where(goldMDP4['Action']=='+OUTER_TURRET',(np.random.rand()*-0.1)+0.05,\n","                     np.where(goldMDP4['Action']=='+DRAGON',(np.random.rand()*-0.1)+0.05,\n","                     np.where(goldMDP4['Action']=='+RIFT_HERALD',(np.random.rand()*-0.1)+0.05,\n","                     np.where(goldMDP4['Action']=='+BARON_NASHOR',(np.random.rand()*-0.1)+0.05,\n","                     np.where(goldMDP4['Action']=='+INNER_TURRET',(np.random.rand()*-0.1)+0.05,\n","                     np.where(goldMDP4['Action']=='+BASE_TURRET',(np.random.rand()*-0.1)+0.05,\n","                     np.where(goldMDP4['Action']=='+INHIBITOR',(np.random.rand()*-0.1)+0.05,\n","                     np.where(goldMDP4['Action']=='+NEXUS_TURRET',(np.random.rand()*-0.1)+0.05,    \n","                     np.where(goldMDP4['Action']=='+ELDER_DRAGON',(np.random.rand()*-0.1)+0.05,\n","                              -0.05))))))))))\n","                              \n","reward = goldMDP4['Reward']\n","goldMDP4[['Action','Reward']].drop_duplicates('Action').sort_values('Reward',ascending=False)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6b68c7e8b2cec8dc31b0ea1080ee97f525ea9865"},"cell_type":"code","source":["alpha = 0.3\n","gamma = 0.9\n","num_episodes = 1000\n","epsilon = 0.2\n","\n","\n","\n","\n","StartMin = 15\n","StartState = 'EVEN'\n","StartAction = None\n","data = goldMDP4\n","\n","Max_Mins = 50\n","start_time = timeit.default_timer()\n","\n","\n","Mdl7 = MCModelv6(data=data, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n","                StartMin = StartMin, StartState=StartState,StartAction=StartAction, \n","                num_episodes = num_episodes, Max_Mins = Max_Mins)\n","\n","elapsed = timeit.default_timer() - start_time\n","\n","print(\"Time taken to run model:\",np.round(elapsed/60,2),\"mins\")\n","print(\"Avg Time taken per episode:\", np.round(elapsed/num_episodes,2),\"secs\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8478dfa761b7bc85748762eacec28c800e1cbfcb"},"cell_type":"code","source":["final_output = Mdl7[2]\n","\n","\n","final_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)]\n","final_output3 = final_output2.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\n","final_output3[['Minute','State','Action','V']]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f9b21835388b5e03d7853572fcc39ce816f7c81b"},"cell_type":"code","source":["final_output = Mdl7[2]\n","V_episodes = Mdl7[3]\n","\n","final_output2 = final_output[(final_output['Minute']==StartMin)&(final_output['State']==StartState)]\n","final_output3 = final_output2.groupby(['Minute','State','Action']).sum().sort_values('V',ascending=False).reset_index()\n","final_output3[['Minute','State','Action','V']]\n","\n","single_action1 = final_output3['Action'][0]\n","single_action2 = final_output3['Action'][len(final_output3)-1]\n","\n","plot_data1 = V_episodes[(V_episodes['Action']==single_action1)]\n","plot_data2 = V_episodes[(V_episodes['Action']==single_action2)]\n","\n","plt.plot(plot_data1['Episode'],plot_data1['V'], label = single_action1)\n","plt.plot(plot_data2['Episode'],plot_data2['V'], label = single_action2)\n","plt.xlabel(\"Epsiode\")\n","plt.ylabel(\"V\")\n","plt.legend()\n","plt.title(\"V by Episode for the Best/Worst Actions given the Current State\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7a4838768e520143e7662f965e5ecd4dbc156ff2"},"cell_type":"code","source":["plt.figure(figsize=(20,10))\n","\n","for actions in V_episodes['Action'].unique():\n","    plot_data = V_episodes[V_episodes['Action']==actions]\n","    plt.plot(plot_data['Episode'],plot_data['V'])\n","plt.xlabel(\"Epsiode\")\n","plt.ylabel(\"V\")\n","plt.title(\"V for each Action by Episode\")\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f36238985251020c48b9737d66785b76f8fe856f"},"cell_type":"markdown","source":["### Conclusion and Collecting Feedback from Players for Rewards\n","\n","\n","I have vastly oversimplified some of the features (such as ‘kills’ not representing the actual amount of kills) and the data is likely not representative of normal matches. However, I hope that this demonstrates an interesting concept clearly and encourages discussion to start about how this could be developed further. \n","\n","First, I will list the main improvements that need to be made before this could be viable for implementation:\n","\n","1. Calculate the MDP using more data that represents the whole player population, not just competitive matches.\n","2. Improve the efficiency of the model so that it can calculate in a more resonable time. Monte Carlo is known for being time consuming so would explore more time efficient algorithms.\n","3. Apply more advanced parameter optimisation to further improve the results.\n","4. Prototype player feedback capture and mapping for a more realistic reward signal.\n","\n","We have introduced rewards for influencing the model output but how is this obtained? Well there are a few ways we could consider but, based on my previous research, I think the best way is to consider a reward that considers both the individual quality of the action AND the quality of transitioning. \n","\n","This becomes more and more complex and not something I will cover here but, in short, we would like to match a player's decision making in which the optimal next decision is dependent on what just occured. For example, if the team kills all players on the enemy team, then they may push to obtain Baron. Our model already takes in to account the probability of events occuring in a sequence so we should also consider a player's decision making in the same way. This idea is drawn from the following research which explains how the feedback can be mapped in more detail (https://www.researchgate.net/publication/259624959_DJ-MC_A_Reinforcement-Learning_Agent_for_Music_Playlist_Recommendation). \n","\n","How we collect this feedback defines how successful our model will be. In my eyes, the end goal of this would be to have real time recommendations for players on the next best decision to make. The player would then be able to select from the top few decisions (ranked in order of success) given the match statistics. This player’s choice can be tracked over multiple games to further learn and understand that player’s preferences. This would also mean that not only could we track the outcome of decisions but would also know what that player attempted to achieve (e.g. tried to take tower but was killed instead) and would open up information for even more advanced analytics.  \n","\n","Of course, an idea like this may cause complications with team mates disagreeing and perhaps take an element out of the game that makes it exciting. But I think something like this could greatly benefit players at a lower or normal skill level where decision making between the players are difficult to communicate clearly. It could also help identify players that are being ‘toxic’ by their actions as teams would look to agree the play via a vote system and it can then be seen whether the toxic player consistently ignores their team mates by their movements instead of following the agreed plans."]},{"metadata":{"_uuid":"2bbdb3f86d79261f166f975347ff5cdd3594d3fb"},"cell_type":"markdown","source":["![Voting Example](https://i.imgur.com/ytz8RRJ.png)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}